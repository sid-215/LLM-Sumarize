{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching batch 1 of 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidha\\AppData\\Local\\Temp\\ipykernel_70416\\2371395959.py:98: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  validated_articles.append(validated.dict())  # Convert model back to simple dict\n"
     ]
    }
   ],
   "source": [
    "from supabase import create_client\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env\n",
    "\n",
    "\n",
    "class ArticleOutput(BaseModel):\n",
    "    ID: int\n",
    "    Topics: List[str]\n",
    "    People: List[str]\n",
    "    Places: List[str]\n",
    "    Translation: str\n",
    "\n",
    "\n",
    "# Supabase Credentials (Replace with your actual Supabase URL and Key)\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "# Create Supabase Client\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "# Function to Get All User-Created Tables\n",
    "# It is just to check the supabase connection. If this returns error, the access to database can be debugged.\n",
    "def get_tables():\n",
    "    try:\n",
    "        response = supabase.table(\"hmtv_data\").select(\"*\").limit(1).execute()\n",
    "        if response.data:\n",
    "            return [\"original_articles\"]  # Replace with actual table names\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tables: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Function to Fetch Articles in Batches\n",
    "# Reads ID and Content   \n",
    "def fetch_article_contents(batch_number=0, batch_size=10):\n",
    "    try:\n",
    "        offset = batch_number * batch_size  # Calculate the offset\n",
    "        response = supabase.table(\"hmtv_data\") \\\n",
    "            .select(\"ID\", \"Content\") \\\n",
    "            .order(\"ID\") \\\n",
    "            .range(offset, offset + batch_size - 1) \\\n",
    "            .execute()\n",
    "        return response.data if response.data else []\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching articles: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Function to Process Articles with a Separator Between Each Article for LLM to process each article independently.\n",
    "# It also adds ID to each article for identification.\n",
    "# This is important as the LLM will process each article independently and we need to keep track of which article is which.\n",
    "# The separator is used to distinguish between different articles in the input text.\n",
    "def format_articles_with_separator(articles):\n",
    "    try:\n",
    "        # Use a compact separator between each article's content\n",
    "        separator = \"\\n###END###\\n\"\n",
    "        \n",
    "        # Format each article to include both ID and Content\n",
    "        formatted_articles = []\n",
    "        for article in articles:\n",
    "            article_text = f\"ID: {article['ID']}\\n{article['Content']}\"\n",
    "            formatted_articles.append(article_text)\n",
    "        \n",
    "        # Join all formatted articles with the separator\n",
    "        formatted_text = separator.join(formatted_articles)\n",
    "        return formatted_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting articles: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "## Uses Pydantic to validate the response from the LLM.\n",
    "## Making sure that the response is in the expected format and contains all the required fields.\n",
    "## This is important as the response will be used to update the database.\n",
    "def validate_cleaned_response(cleaned_response: str):\n",
    "    try:\n",
    "        # First, parse the cleaned JSON string\n",
    "        parsed_data = json.loads(cleaned_response)  # List of dicts\n",
    "        \n",
    "        validated_articles = []\n",
    "\n",
    "        for idx, article in enumerate(parsed_data):\n",
    "            try:\n",
    "                # Validate each article using Pydantic\n",
    "                validated = ArticleOutput(**article)\n",
    "                validated_articles.append(validated.dict())  # Convert model back to simple dict\n",
    "            except ValidationError as ve:\n",
    "                print(f\"Validation error at index {idx}: {ve}\")\n",
    "\n",
    "        return validated_articles\n",
    "\n",
    "    except json.JSONDecodeError as je:\n",
    "        print(f\"JSON decoding failed: {je}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# This the core function that processes the articles using the Gemini 2.0 Flash API.\n",
    "# Exceptions are used to log where exactly are we facing issues.\n",
    "def process_with_custom_prompt(articles, custom_prompt):\n",
    "    try:\n",
    "        formatted_text = format_articles_with_separator(articles)\n",
    "        prompt = custom_prompt.format(articles=formatted_text)\n",
    "        response = model.generate_content(prompt)\n",
    "\n",
    "        cleaned_response = response.text.replace('```json', '').replace('```', '').strip()\n",
    "\n",
    "        if not cleaned_response:\n",
    "            print(\"Received an empty or invalid response.\")\n",
    "            return []\n",
    "\n",
    "        \n",
    "        validated_data = validate_cleaned_response(cleaned_response)\n",
    "\n",
    "        if not validated_data:\n",
    "            print(\"Validation failed: No valid articles found.\")\n",
    "            return []\n",
    "\n",
    "        return validated_data  # Safe validated articles ready for DataFrame or further processing\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    processed_df = pd.DataFrame()\n",
    "\n",
    "    batch_size = 10\n",
    "    total_batches = 50  # Load batch_size * total_batches articles (batch_size at a time)\n",
    "\n",
    "    for batch_number in range(total_batches):\n",
    "        print(f\"Fetching batch {batch_number + 1} of {total_batches}...\")\n",
    "        articles = fetch_article_contents(batch_number, batch_size)\n",
    "\n",
    "        if articles:\n",
    "            # Example custom prompt (you can pass your own)\n",
    "            custom_prompt = \"\"\"\n",
    "            The following are multiple articles separated by '###END###'\n",
    "            For each article, I need you to generate a structured json output.\n",
    "            Given the following articles extract the most relevant topics. \n",
    "            Focus on high-level topics that capture the primary narrative of the article\n",
    "            Try keeping the topics concise and relevant to the article's content.\n",
    "            Keep the topic within 2 words but you can expand only if you think it is necessary.\n",
    "            Topics should also contain any entities, organizations mentioned.\n",
    "            The topics should be a list of keywords or phrases that summarize the main themes or subjects of the article.\n",
    "            Topics may also include events occurred in the article, but should not be limited to them. \n",
    "            The topics should primarily describe the core issues or themes discussed in the article.\n",
    "            Do not include information about legal procedure, administrative details or any routine or general details.\n",
    "            Identify the main themes and entities involved, excluding minor or tangential events.\n",
    "            Also identify all the people and all the places referred to in each article. \n",
    "            The output for Topics, Places, People should be in English language only.\n",
    "            Transalate the article into English and include it in the output at Translation key.\n",
    "            I am also providing the numerical ID with each article.\n",
    "            Do not give backticks in output as I need to parse your output into json.\n",
    "            Do not explain your reasoning. Just give the output\n",
    "            Articles:\n",
    "            {articles}\n",
    "            Output should be a json array with each element containing the following keys:\n",
    "            - 'ID': only numeric id of the article that has been provided\n",
    "            - 'Topics': a list of topics\n",
    "            - 'People': a list of people    \n",
    "            - 'Places': a list of places\n",
    "            - 'Translation' : english translation of the article\n",
    "            \"\"\"\n",
    "            processed_data = process_with_custom_prompt(articles, custom_prompt)\n",
    "\n",
    "\n",
    "            if processed_data:\n",
    "                    # Assuming processed_data is a dict or list of dicts\n",
    "                    batch_df = pd.DataFrame(processed_data)  # Convert processed data to DataFrame\n",
    "                    processed_df = pd.concat([processed_df, batch_df], ignore_index=True)\n",
    "                    \n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "df = processed_df\n",
    "    \n",
    "df['Topics'] = df['Topics'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x).str.replace('[', '').str.replace(']', '')\n",
    "df['People'] = df['People'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x).str.replace('[', '').str.replace(']', '')\n",
    "df['Places'] = df['Places'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x).str.replace('[', '').str.replace(']', '')\n",
    "\n",
    "\n",
    "id_list = df['ID'].tolist()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    id_value = row['ID']\n",
    "    \n",
    "    # Create the update data\n",
    "    update_data = {\n",
    "        'Topics': row['Topics'],\n",
    "        'People': row['People'],\n",
    "        'Places': row['Places'],\n",
    "        'Translation': row['Translation']\n",
    "    }\n",
    "    \n",
    "    # Update the Supabase record\n",
    "    supabase.table('hmtv_data') \\\n",
    "        .update(update_data) \\\n",
    "        .eq('ID', id_value) \\\n",
    "        .execute()\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Topics</th>\n",
       "      <th>People</th>\n",
       "      <th>Places</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Formula E Race, ACB Investigation, KTR, ED Not...</td>\n",
       "      <td>KTR, Aravind Kumar, BLN Reddy</td>\n",
       "      <td>Telangana, Telangana Bhavan</td>\n",
       "      <td>Formula E Race Case: A high drama took place a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sankranti Holidays, School Calendar, Holiday S...</td>\n",
       "      <td>Krishna Reddy</td>\n",
       "      <td>Telangana, AP</td>\n",
       "      <td>Sankranti Holidays 2025: Sankranti festival is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NIT Warangal, Non-Teaching Jobs, Recruitment</td>\n",
       "      <td></td>\n",
       "      <td>Warangal</td>\n",
       "      <td>NIT Warangal Recruitment: Notification has bee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                             Topics  \\\n",
       "0   1  Formula E Race, ACB Investigation, KTR, ED Not...   \n",
       "1   2  Sankranti Holidays, School Calendar, Holiday S...   \n",
       "2   3       NIT Warangal, Non-Teaching Jobs, Recruitment   \n",
       "\n",
       "                          People                       Places  \\\n",
       "0  KTR, Aravind Kumar, BLN Reddy  Telangana, Telangana Bhavan   \n",
       "1                  Krishna Reddy                Telangana, AP   \n",
       "2                                                    Warangal   \n",
       "\n",
       "                                         Translation  \n",
       "0  Formula E Race Case: A high drama took place a...  \n",
       "1  Sankranti Holidays 2025: Sankranti festival is...  \n",
       "2  NIT Warangal Recruitment: Notification has bee...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"response = supabase.table('hmtv_data') \\\n",
    "    .select('*') \\\n",
    "    .in_('ID', id_list) \\\n",
    "    .execute()\n",
    "supabase_df = pd.DataFrame(response.data)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"id_list = df['ID'].tolist()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    id_value = row['ID']\n",
    "    \n",
    "    # Create the update data\n",
    "    update_data = {\n",
    "        'Topics': row['Topics'],\n",
    "        'People': row['People'],\n",
    "        'Places': row['Places'],\n",
    "        'Translation': row['Translation']\n",
    "    }\n",
    "    \n",
    "    # Update the Supabase record\n",
    "    supabase.table('hmtv_data') \\\n",
    "        .update(update_data) \\\n",
    "        .eq('ID', id_value) \\\n",
    "        .execute()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
